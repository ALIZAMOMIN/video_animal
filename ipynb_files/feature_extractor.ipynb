{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e1b70",
   "metadata": {
    "executionInfo": {
     "elapsed": 190,
     "status": "ok",
     "timestamp": 1755436912192,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "805e1b70"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1wiobjP909nP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2359,
     "status": "ok",
     "timestamp": 1755436914557,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "1wiobjP909nP",
    "outputId": "a450063c-664d-4697-deb1-e983eccd06e9"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tr6OOE93iLwy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1755436914640,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "tr6OOE93iLwy",
    "outputId": "fa159c42-cc62-442b-ad79-0718ed64f3d7"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# correct path\n",
    "dataset_path = \"/content/drive/MyDrive/dog_dataset_split/train\"\n",
    "\n",
    "# list label types (aggressive / notaggressive)\n",
    "label_types = os.listdir(dataset_path)\n",
    "print(label_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jRWfk_Xq2pmZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1755436914665,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "jRWfk_Xq2pmZ",
    "outputId": "5ab01f9e-0ad3-49f3-b070-3c752e1c6517"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_dir = '/content/drive/MyDrive/dog_dataset_split/train'\n",
    "class_folders = os.listdir(train_dir)\n",
    "print(class_folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610d516",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1755436914751,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "b610d516",
    "outputId": "808361dc-599e-4556-f164-40bda1e21011"
   },
   "outputs": [],
   "source": [
    "rooms = []\n",
    "for item in class_folders:\n",
    "    class_folder = os.path.join(train_dir, item)\n",
    "    if os.path.isdir(class_folder):  # Only proceed if it's a folder\n",
    "        all_files = os.listdir(class_folder)\n",
    "        for fname in all_files:\n",
    "            file_path = os.path.join(class_folder, fname)\n",
    "            rooms.append((item, file_path))\n",
    "\n",
    "# Build a dataframe\n",
    "import pandas as pd\n",
    "train_df = pd.DataFrame(rooms, columns=['tag', 'video_path'])\n",
    "print(train_df.head())\n",
    "print(train_df.tail())\n",
    "\n",
    "df = train_df.loc[:,['video_path','tag']]\n",
    "df\n",
    "df.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a64347",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 103,
     "status": "ok",
     "timestamp": 1755436914858,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "81a64347",
    "outputId": "1e5425ea-fe6a-4c5a-df22-bba29d7689b3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "test_dir = '/content/drive/MyDrive/dog_dataset_split/test'\n",
    "activity_types = os.listdir(test_dir)\n",
    "print(\"Types of activities found:\", activity_types)\n",
    "\n",
    "rooms = []\n",
    "\n",
    "for activity in activity_types:\n",
    "    activity_folder = os.path.join(test_dir, activity)\n",
    "    if os.path.isdir(activity_folder):  # Only if it's a folder\n",
    "        for fname in os.listdir(activity_folder):\n",
    "            # Absolute file path on Drive\n",
    "            full_path = os.path.join(activity_folder, fname)\n",
    "            # OR if you want a relative path: os.path.join('dog_dataset_split/test', activity, fname)\n",
    "            rooms.append((activity, full_path))\n",
    "\n",
    "# Build a dataframe\n",
    "test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_path'])\n",
    "print(test_df.head())\n",
    "print(test_df.tail())\n",
    "\n",
    "# If you specifically need only two columns:\n",
    "df = test_df.loc[:, ['video_path', 'tag']]\n",
    "df.to_csv('test.csv', index=False)\n",
    "print(\"CSV saved as test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df78e8fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1755436914923,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "df78e8fa",
    "outputId": "81109418-3ece-443c-dd31-1d5092246981"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/content/train.csv\")\n",
    "test_df = pd.read_csv(\"/content/test.csv\")\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6132e5f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "executionInfo": {
     "elapsed": 4249,
     "status": "ok",
     "timestamp": 1755436919175,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "6132e5f6",
    "outputId": "17a7a777-fe13-4bbb-a7bd-d1d02cabb0da"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m train_df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m  \u001b[38;5;66;03m#convert classtags to tag encoding\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aliza\\Desktop\\og_behaviour\\.venv\\lib\\site-packages\\torch\\__init__.py:278\u001b[0m\n\u001b[0;32m    274\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    276\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 278\u001b[0m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;66;03m# Libraries can either be in path/nvidia/lib_folder/lib or path/lib_folder/lib\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aliza\\Desktop\\og_behaviour\\.venv\\lib\\site-packages\\torch\\__init__.py:254\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    252\u001b[0m is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[1;32m--> 254\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mkernel32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadLibraryExW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0x00001100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m     last_error \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mget_last_error()\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m126\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train label encoding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "train_df=pd.read_csv('train.csv')\n",
    " #convert classtags to tag encoding\n",
    "train_df = train_df.dropna(subset=['tag']).reset_index(drop=True)\n",
    "train_df['tag'] = train_df['tag'].map({'aggressive': 1, 'not_aggressive': 0})\n",
    "tag= train_df['tag'].astype(int)\n",
    "tag=torch.tensor(tag)\n",
    "#print(tag,type(tag))\n",
    "train_df.head()\n",
    "\n",
    "\n",
    "#for test\n",
    "\n",
    "\n",
    "test_df=pd.read_csv('test.csv')\n",
    "#convert classtags to tag encoding\n",
    "test_df = test_df.dropna(subset=['tag']).reset_index(drop=True)\n",
    "test_df['tag'] = test_df['tag'].map({'aggressive': 1, 'not_aggressive': 0})\n",
    "tag= test_df['tag'].astype(int)\n",
    "tag=torch.tensor(tag)\n",
    "print(tag,type(tag))\n",
    "test_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc03a6f5",
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1755444017141,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "dc03a6f5"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "MAX_SEQ_LENGTH = 300 # Maximum number of frames to use per video\n",
    "NUM_FEATURES = 1280 # Number of features extracted by MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fHqwZHvD4Myb",
   "metadata": {
    "id": "fHqwZHvD4Myb"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf44088",
   "metadata": {
    "executionInfo": {
     "elapsed": 78,
     "status": "ok",
     "timestamp": 1755440363154,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "ecf44088"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "def build_feature_extractor():\n",
    "    mobilenet = models.mobilenet_v2(weights=\"IMAGENET1K_V1\")\n",
    "    mobilenet.eval()\n",
    "    # Remove classifier head, add adaptive pool and flatten\n",
    "    feature_extractor = nn.Sequential(\n",
    "        *list(mobilenet.children())[:-1],      # all but classifier\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),         # global avg pool\n",
    "        nn.Flatten()\n",
    "    )\n",
    "    return feature_extractor\n",
    "\n",
    "# Usage:\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7571661e",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1755444092313,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "7571661e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import frame_constructor as fc  # Assuming this is your feature extraction module\n",
    "\n",
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_path\"].values.tolist()\n",
    "    labels = torch.tensor(df[\"tag\"].values, dtype=torch.long)\n",
    "\n",
    "    # Preallocate tensors for whole dataset\n",
    "    frame_masks = torch.zeros((num_samples, MAX_SEQ_LENGTH), dtype=torch.bool)\n",
    "    frame_features = torch.zeros((num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=torch.float32)\n",
    "\n",
    "    for idx, path in enumerate(video_paths): #loop for each video\n",
    "        # Use absolute path if present, else combine with root_dir\n",
    "        if os.path.isabs(path):\n",
    "            video_path = path\n",
    "        else:\n",
    "            video_path = os.path.join(root_dir, path)\n",
    "\n",
    "        frames = fc.frame_cons(video_path)\n",
    "        # frames shape: (video_length, H, W, C), numpy array # eg:300, 224,224,3\n",
    "\n",
    "        video_length = frames.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length) #change to min length if video is shorter/larger than MAX_SEQ_LENGTH\n",
    "\n",
    "        # Process frames one by one (or batch if possible:not doing here)\n",
    "        #per video\n",
    "        temp_features = torch.zeros((MAX_SEQ_LENGTH, NUM_FEATURES), dtype=torch.float32) #20,2048\n",
    "\n",
    "        for i in range(length): #for each frame in the video\n",
    "            # Convert frame to tensor and permute channels if needed for model\n",
    "            #print(\"Feature shape:\", temp_features.shape)\n",
    "            frame = frames[i]  # shape (H, W, C), numpy\n",
    "            frame_tensor = torch.from_numpy(frame).permute(2, 0, 1).unsqueeze(0).float()  # (1, C, H, W)\n",
    "\n",
    "            # Normalize as needed for MobileNetV2\n",
    "            frame_tensor = frame_tensor / 255.0\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406], device=frame_tensor.device).view(1, 3, 1, 1)\n",
    "            std = torch.tensor([0.229, 0.224, 0.225], device=frame_tensor.device).view(1, 3, 1, 1)\n",
    "            frame_tensor = (frame_tensor - mean) / std\n",
    "\n",
    "            # Extract features with model in eval mode, no grad\n",
    "            with torch.no_grad():\n",
    "                #mobile_net_v2 expects input shape (1, 3, IMG_SIZE, IMG_SIZE)\n",
    "                #output shape of mobile_net_v2 is (batch_size, 1280) after flattning\n",
    "                feat = feature_extractor(frame_tensor)\n",
    "                #print(\"Feat shape:\", feat.shape)  # expect output: (1, NUM_FEATURES)\n",
    "            temp_features[i] = feat.squeeze(0)  # feat.squeeze(0) removes the batch dimension, making it (NUM_FEATURES,)\n",
    "\n",
    "        # Assign features and mask\n",
    "        frame_features[idx] = temp_features # Store features for this video\n",
    "        frame_masks[idx, :length] = True    # Mark valid frames as True in the mask\n",
    "        print('done for ', idx)\n",
    "        #print(\"Frame features shape:\", frame_features.shape)\n",
    "        #print(\"Frame masks shape:\", frame_masks.shape)\n",
    "\n",
    "    return (frame_features, frame_masks), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-Z6qUFE44Cic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 423503,
     "status": "ok",
     "timestamp": 1755449031532,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "-Z6qUFE44Cic",
    "outputId": "7a623753-56eb-4bf8-f104-a4a5d700b7dd"
   },
   "outputs": [],
   "source": [
    "(train_features, train_masks), train_labels = prepare_all_videos(train_df, \" \")\n",
    "(test_features, test_masks), test_labels = prepare_all_videos(test_df, \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HcXy2-md5qNd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1755449031560,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "HcXy2-md5qNd",
    "outputId": "977e91be-af8e-4eac-c78f-613acb2ec000"
   },
   "outputs": [],
   "source": [
    "print((train_masks.sum(dim=1) == 0).sum().item(), \"samples have zero valid frames in training data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zhfZWshv5slJ",
   "metadata": {
    "executionInfo": {
     "elapsed": 75,
     "status": "ok",
     "timestamp": 1755449031643,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "zhfZWshv5slJ"
   },
   "outputs": [],
   "source": [
    "#lstm sequence model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMSequenceModel(nn.Module):\n",
    "    def __init__(self, num_features, max_seq_length, num_classes):\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=num_features, hidden_size=16, num_layers=1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=16, hidden_size=8, num_layers=1, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc1 = nn.Linear(8, 8)\n",
    "        self.fc2 = nn.Linear(8, num_classes)\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (batch_size, seq_len, num_features)\n",
    "        lengths = mask.sum(dim=1) if mask is not None else torch.full((x.size(0),), x.size(1), dtype=torch.long).to(x.device)\n",
    "        lengths = torch.clamp(lengths, min=1)  # Prevent zero lengths\n",
    "        # Pack padded sequence for the LSTM\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (h1, _) = self.lstm1(packed)\n",
    "        packed_output, (h2, _) = self.lstm2(packed_output)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True, total_length=x.size(1))\n",
    "\n",
    "        # Get the last valid output in each sequence according to actual length\n",
    "        idx = (lengths - 1).unsqueeze(1).unsqueeze(2).expand(-1, 1, output.shape[2])\n",
    "        last_outputs = output.gather(1, idx).squeeze(1)\n",
    "        x = self.dropout(last_outputs)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        #print(x.shape) 16,2\n",
    "        return x  # logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "si6hFqIG5_eD",
   "metadata": {
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1755449031921,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "si6hFqIG5_eD"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(train_features, train_masks, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(test_features, test_masks, test_labels)  # if using validation set\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NBHUwaua5_ZH",
   "metadata": {
    "executionInfo": {
     "elapsed": 143,
     "status": "ok",
     "timestamp": 1755449032083,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "NBHUwaua5_ZH"
   },
   "outputs": [],
   "source": [
    "NUM_FEATURES = train_features.shape[2]\n",
    "MAX_SEQ_LENGTH = train_features.shape[1]\n",
    "NUM_CLASSES = len(torch.unique(train_labels))\n",
    "\n",
    "model = LSTMSequenceModel(num_features=NUM_FEATURES, max_seq_length=MAX_SEQ_LENGTH, num_classes=NUM_CLASSES)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ZfhAQd6GsV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 78389,
     "status": "ok",
     "timestamp": 1755449110496,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "25ZfhAQd6GsV",
    "outputId": "dd437c53-ca25-445c-f7c9-04c402bf143d"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        features, masks, labels = [x.to(device) for x in batch]  # Unpack batch\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(features, masks)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * features.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "    train_acc = train_correct / train_total\n",
    "    avg_train_loss = train_loss / train_total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            features, masks, labels = [x.to(device) for x in batch]\n",
    "            logits = model(features, masks)\n",
    "            preds = logits.argmax(1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Train loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict().copy()\n",
    "\n",
    "# Restore best model\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jiFYRq6IDKbO",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1755449110540,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "jiFYRq6IDKbO"
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), \"model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JB2BCAccEld8",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1755449110565,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "JB2BCAccEld8"
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': EPOCHS,  # current epoch index (0-based)\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, 'checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1oS5buET6M-E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1755449110591,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "1oS5buET6M-E",
    "outputId": "3190e638-1e31-47f5-af38-1c1ee4b63aea"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        features, masks, labels = [x.to(device) for x in batch]\n",
    "        logits = model(features, masks)\n",
    "        preds = logits.argmax(1)\n",
    "        test_correct += (preds == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "test_acc = test_correct / test_total\n",
    "print(f\"Test accuracy: {test_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KPLBQjqhDvMI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1755449110991,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "KPLBQjqhDvMI",
    "outputId": "ccbfcb21-cd34-4ab7-fdbf-ffad5ab08b9d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import frame_constructor as fc  # Your feature extraction module\n",
    "\n",
    "# Define constants (set these as per your setup)\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "MAX_SEQ_LENGTH = 400 # Maximum number of frames to use per video\n",
    "NUM_FEATURES = 1280 # Number of features extracted by MobileNetV2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hardcoded class vocabulary corresponding to your label encoding\n",
    "class_vocab = [\"not_aggressive\", \"aggressive\"]  # index 0 = not_aggressive, 1 = aggressive\n",
    "\n",
    "# Your loaded PyTorch models\n",
    "# feature_extractor: feature extraction model\n",
    "# model: your LSTMSequenceModel or equivalent for classification\n",
    "# Make sure they are both on the correct device\n",
    "feature_extractor.to(device).eval()\n",
    "model.to(device).eval()\n",
    "\n",
    "def prepare_single_video(video_path):\n",
    "    # Load frames using your frame constructor function\n",
    "    frames = fc.frame_cons(video_path)  # (num_frames, H, W, C), numpy array\n",
    "\n",
    "    frame_mask = torch.zeros((1, MAX_SEQ_LENGTH), dtype=torch.bool)\n",
    "    frame_features = torch.zeros((1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=torch.float32)\n",
    "\n",
    "    length = min(MAX_SEQ_LENGTH, frames.shape[0])\n",
    "\n",
    "    for j in range(length):\n",
    "        frame = frames[j]\n",
    "        frame_tensor = torch.from_numpy(frame).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "        frame_tensor = (frame_tensor - mean) / std\n",
    "        frame_tensor = frame_tensor.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feat = feature_extractor(frame_tensor)  # output shape (1, NUM_FEATURES)\n",
    "        frame_features[0, j, :] = feat.squeeze(0).cpu()\n",
    "\n",
    "    frame_mask[0, :length] = 1\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "def sequence_prediction(video_path):\n",
    "    frame_features, frame_mask = prepare_single_video(video_path)\n",
    "    frame_features = frame_features.to(device)\n",
    "    frame_mask = frame_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(frame_features, frame_mask)\n",
    "        probabilities = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "\n",
    "    print(f\"Prediction for video: {video_path}\")\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i]*100:5.2f}%\")\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "# Example usage:\n",
    "video_path = \"/content/sample_video.mp4\"\n",
    "sequence_prediction(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6vV2Nt5rZm8Y",
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1755449111127,
     "user": {
      "displayName": "aliza momin",
      "userId": "01893634910837385009"
     },
     "user_tz": -330
    },
    "id": "6vV2Nt5rZm8Y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
