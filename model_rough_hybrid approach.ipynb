{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2259c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yolo rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c14e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv lastm code Arch \n",
    "#make mobilenet+use arch \n",
    "#maek 2streamacrch \n",
    "#train on bounding box\n",
    "#train on full frame\n",
    "#combine output of both\n",
    "#this is called hybrid approach twostram or multistram architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826311c2",
   "metadata": {},
   "source": [
    "Hybrid approach (common in research)\n",
    "Feed both: a cropped bounding box stream (for subject-focused motion) and a full-frame stream (for context).\n",
    "\n",
    "This is called a two-stream or multi-stream architecture — common in action recognition models like I3D, SlowFast, or VideoMAE.\n",
    "\n",
    "1)Example shapes for batch size B:\n",
    "\n",
    "cropped_seq  → [B, T, 3, H, W]\n",
    "full_seq     → [B, T, 3, H, W]\n",
    "label        → [B]\n",
    "\n",
    "2)Feed both streams at once:\n",
    "outputs = model(cropped_seq, full_seq)  # both go through their own MobileNet+ConvLSTM\n",
    "loss = criterion(outputs, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02ef305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset,TensorDataset\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4655f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    "Raw dataset\n",
    "\n",
    "YOLO detection\n",
    "\n",
    "Save processed (cropped) dataset\n",
    "\n",
    "Train/test split\n",
    "\n",
    "On-the-fly augmentations during training\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a64cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, ..., False, False, False],\n",
       "       [ True, False, False, ..., False, False, False],\n",
       "       [ True, False, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False,  True, False],\n",
       "       [False, False, False, ..., False,  True, False],\n",
       "       [False, False, False, ..., False, False,  True]], shape=(294, 16))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dog_dataset_full.csv\")\n",
    "df['label'] = df['label'].map({'aggressive': 1, 'notaggressive': 0}) #encoding labels\n",
    "\n",
    "if 'action' in df.columns:\n",
    "    df = pd.get_dummies(df, columns=['action']) # One-hot encoding for 'action' column\n",
    "\n",
    "\n",
    "col_names = [col for col in df.columns if col not in ['video_path', 'label']] #collecting col names\n",
    "\n",
    "#convert to pytorch tensor\n",
    "X = torch.tensor(df[col_names].values, dtype=torch.float32)\n",
    "y = torch.tensor(df['label'].values, dtype=torch.long)\n",
    "\n",
    "#df[col_names].values #extract features values as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af57756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "      <th>action_barks_clips</th>\n",
       "      <th>action_chasing_toy</th>\n",
       "      <th>action_chewing</th>\n",
       "      <th>action_digging</th>\n",
       "      <th>action_drinking</th>\n",
       "      <th>action_eating_food</th>\n",
       "      <th>action_growling_clips</th>\n",
       "      <th>action_howling</th>\n",
       "      <th>action_jump</th>\n",
       "      <th>action_panting</th>\n",
       "      <th>action_playing</th>\n",
       "      <th>action_rolling</th>\n",
       "      <th>action_sitting</th>\n",
       "      <th>action_sleeping</th>\n",
       "      <th>action_teeth_chattring</th>\n",
       "      <th>action_wagging_tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\aliza\\Desktop\\og_behaviour\\dataset_2\\...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\aliza\\Desktop\\og_behaviour\\dataset_2\\...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\aliza\\Desktop\\og_behaviour\\dataset_2\\...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\aliza\\Desktop\\og_behaviour\\dataset_2\\...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\aliza\\Desktop\\og_behaviour\\dataset_2\\...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          video_path  label  \\\n",
       "0  C:\\Users\\aliza\\Desktop\\og_behaviour\\dataset_2\\...    1.0   \n",
       "1  C:\\Users\\aliza\\Desktop\\og_behaviour\\dataset_2\\...    1.0   \n",
       "2  C:\\Users\\aliza\\Desktop\\og_behaviour\\dataset_2\\...    1.0   \n",
       "3  C:\\Users\\aliza\\Desktop\\og_behaviour\\dataset_2\\...    1.0   \n",
       "4  C:\\Users\\aliza\\Desktop\\og_behaviour\\dataset_2\\...    1.0   \n",
       "\n",
       "   action_barks_clips  action_chasing_toy  action_chewing  action_digging  \\\n",
       "0                True               False           False           False   \n",
       "1                True               False           False           False   \n",
       "2                True               False           False           False   \n",
       "3                True               False           False           False   \n",
       "4                True               False           False           False   \n",
       "\n",
       "   action_drinking  action_eating_food  action_growling_clips  action_howling  \\\n",
       "0            False               False                  False           False   \n",
       "1            False               False                  False           False   \n",
       "2            False               False                  False           False   \n",
       "3            False               False                  False           False   \n",
       "4            False               False                  False           False   \n",
       "\n",
       "   action_jump  action_panting  action_playing  action_rolling  \\\n",
       "0        False           False           False           False   \n",
       "1        False           False           False           False   \n",
       "2        False           False           False           False   \n",
       "3        False           False           False           False   \n",
       "4        False           False           False           False   \n",
       "\n",
       "   action_sitting  action_sleeping  action_teeth_chattring  \\\n",
       "0           False            False                   False   \n",
       "1           False            False                   False   \n",
       "2           False            False                   False   \n",
       "3           False            False                   False   \n",
       "4           False            False                   False   \n",
       "\n",
       "   action_wagging_tail  \n",
       "0                False  \n",
       "1                False  \n",
       "2                False  \n",
       "3                False  \n",
       "4                False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0a8037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "185237fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 294\n",
      "Number of batches: 19\n",
      "Feature shape: torch.Size([294, 16])\n",
      "Label shape: torch.Size([294])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "print(f\"Feature shape: {X.shape}\")\n",
    "print(f\"Label shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce04a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##different \n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read csv\n",
    "df = pd.read_csv(\"dog_dataset_full.csv\")\n",
    "\n",
    "# map labels to int\n",
    "df['label'] = df['label'].map({'aggressive': 1, 'notaggressive': 0})\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}\")\n",
    "\n",
    "\n",
    "# MobileNetV2 feature extractor\n",
    "mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "feature_extractor = nn.Sequential(*list(mobilenet.children())[:-1])  # remove classifier\n",
    "feature_extractor.eval()\n",
    "\n",
    "class VideoFeatureDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, transform=None, max_seq_len=32):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        video_path = os.path.join(self.root_dir, row[\"video_path\"])\n",
    "        label = row[\"label\"]\n",
    "\n",
    "        frames = self.load_video(video_path)\n",
    "        frames = self.pad_or_truncate(frames, self.max_seq_len)\n",
    "\n",
    "        return frames, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "    def load_video(self, path):\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, (224,224))\n",
    "\n",
    "            frame = torch.tensor(frame, dtype=torch.float32).permute(2,0,1)/255.0\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "\n",
    "            # extract features\n",
    "            with torch.no_grad():\n",
    "                f = feature_extractor(frame.unsqueeze(0))  # (1,1280,7,7)\n",
    "                f = torch.mean(f, dim=[2,3]).squeeze(0)   # (1280,)\n",
    "            frames.append(f)\n",
    "        cap.release()\n",
    "        return torch.stack(frames) if frames else torch.zeros((0,1280))\n",
    "\n",
    "    def pad_or_truncate(self, frames, max_len):\n",
    "        length = frames.shape[0]\n",
    "        if length >= max_len:\n",
    "            return frames[:max_len]\n",
    "        else:\n",
    "            pad = torch.zeros((max_len - length, frames.shape[1]))\n",
    "            return torch.cat([frames, pad], dim=0)\n",
    "        \n",
    "\n",
    "train_transforms = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "])\n",
    "\n",
    "train_dataset = VideoFeatureDataset(train_df, root_dir=\"videos\", transform=train_transforms, max_seq_len=32)\n",
    "val_dataset   = VideoFeatureDataset(val_df,   root_dir=\"videos\", transform=None, max_seq_len=32)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=1280, hidden_dim=512, num_layers=2, num_classes=2):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)  # *2 for bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, feature_dim)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # last timestep\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LSTMClassifier(input_dim=1280, hidden_dim=512, num_layers=2, num_classes=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train loss {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb841289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e315ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader\n",
    "#dataset should return paired sequences\n",
    "class TwoStreamDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, crop_videos, full_videos, labels):\n",
    "        self.crop_videos = crop_videos\n",
    "        self.full_videos = full_videos\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        crop_seq = self.crop_videos[idx]  # Tensor [T, C, H, W]\n",
    "        full_seq = self.full_videos[idx]  # Tensor [T, C, H, W]\n",
    "        label = self.labels[idx]\n",
    "        return crop_seq, full_seq, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc51576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv lstm code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=input_dim + hidden_dim,\n",
    "            out_channels=4 * hidden_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h_cur, c_cur = hidden\n",
    "        combined = torch.cat([x, h_cur], dim=1)  # [B, C, H, W]\n",
    "        conv_output = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.chunk(conv_output, 4, dim=1)\n",
    "\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        H, W = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, H, W, device='cuda'),\n",
    "                torch.zeros(batch_size, self.hidden_dim, H, W, device='cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mobilenet+convlstm\n",
    "import torchvision.models as models\n",
    "\n",
    "class MobileNetConvLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "        self.feature_extractor = mobilenet.features  # keep spatial feature maps\n",
    "        feature_channels = 1280  # last layer channels\n",
    "        \n",
    "        self.convlstm = ConvLSTMCell(input_dim=feature_channels, hidden_dim=hidden_dim, kernel_size=3)\n",
    "        \n",
    "        # Classifier\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x_seq):\n",
    "        \"\"\"\n",
    "        x_seq: [B, T, C, H, W]\n",
    "        \"\"\"\n",
    "        b, t, c, h, w = x_seq.shape\n",
    "        h_state, c_state = self.convlstm.init_hidden(b, (h//32, w//32))  # MobileNet downsamples by 32\n",
    "\n",
    "        for frame in range(t):\n",
    "            features = self.feature_extractor(x_seq[:, frame])  # [B, 1280, H', W']\n",
    "            h_state, c_state = self.convlstm(features, (h_state, c_state))\n",
    "\n",
    "        pooled = self.pool(h_state).view(b, -1)\n",
    "        return self.fc(pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d19522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2stream\n",
    "class TwoStreamMobileNetConvLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.stream1 = MobileNetConvLSTM(hidden_dim, num_classes)\n",
    "        self.stream2 = MobileNetConvLSTM(hidden_dim, num_classes)\n",
    "        self.fc_final = nn.Linear(num_classes*2, num_classes)\n",
    "\n",
    "    def forward(self, seq1, seq2):\n",
    "        out1 = self.stream1(seq1)  # cropped\n",
    "        out2 = self.stream2(seq2)  # full frame\n",
    "        combined = torch.cat([out1, out2], dim=1)\n",
    "        return self.fc_final(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trianing loop\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MobileNetConvLSTM(hidden_dim=256, num_classes=10).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        videos, labels = batch  # videos: [B, T, C, H, W]\n",
    "        videos, labels = videos.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e575e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other way\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# ConvLSTM module (simplified version)\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size=(3,3), num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, T, features]\n",
    "        output, _ = self.lstm(x)\n",
    "        return output[:, -1, :]  # last time step\n",
    "\n",
    "# Stream network: MobileNetV2 → ConvLSTM\n",
    "class StreamNet(nn.Module):\n",
    "    def __init__(self, mobilenet_weights=True):\n",
    "        super().__init__()\n",
    "        mobilenet = models.mobilenet_v2(pretrained=mobilenet_weights)\n",
    "        mobilenet.classifier = nn.Identity()  # remove final classification layer\n",
    "        self.cnn = mobilenet\n",
    "        self.temporal = ConvLSTM(input_dim=1280, hidden_dim=512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, T, C, H, W]\n",
    "        B, T, C, H, W = x.size()\n",
    "        feats = []\n",
    "        for t in range(T):\n",
    "            f = self.cnn(x[:, t])  # [B, 1280]\n",
    "            feats.append(f)\n",
    "        feats = torch.stack(feats, dim=1)  # [B, T, 1280]\n",
    "        return self.temporal(feats)\n",
    "\n",
    "# Two-stream fusion network\n",
    "class TwoStreamNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.stream1 = StreamNet()\n",
    "        self.stream2 = StreamNet()\n",
    "        self.fc = nn.Linear(512*2, num_classes)\n",
    "\n",
    "    def forward(self, crop_seq, full_seq):\n",
    "        feat1 = self.stream1(crop_seq)\n",
    "        feat2 = self.stream2(full_seq)\n",
    "        fused = torch.cat((feat1, feat2), dim=1)\n",
    "        return self.fc(fused)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4373059",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = TwoStreamNet(num_classes=10).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for (crop_seq, full_seq, labels) in train_loader:\n",
    "        crop_seq, full_seq, labels = crop_seq.to(device), full_seq.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(crop_seq, full_seq)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a54740a",
   "metadata": {},
   "outputs": [],
   "source": [
    " MobileNetV2 + Transfer Learning (Frame-based) version in PyTorch.\n",
    "This will:\n",
    "\n",
    "Extract frames from videos\n",
    "\n",
    "Fine-tune MobileNetV2 for binary classification (Aggressive / Not Aggressive)\n",
    "\n",
    "Average predictions from 3–5 frames per video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
